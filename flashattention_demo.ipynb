{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfeUc3KX2nA2"
   },
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVw5ih_k2q74"
   },
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash_attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ANUD9EQ2q-B"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from flash_attn import FlashAttention\n",
    "from flash_attn.modules.mha import FlashSelfAttention, FlashAttention2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61E1gLHn2rAU"
   },
   "outputs": [],
   "source": [
    "# Initialize BERT Tokenizer and sample text data\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = [\n",
    "    \"FlashAttention is a fast and memory-efficient attention mechanism.\",\n",
    "    \"FlashAttention-2 optimizes parallelism and work partitioning.\"\n",
    "]\n",
    "# Tokenize the input text and move tensors to the GPU\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJd4WwJW2rCl"
   },
   "outputs": [],
   "source": [
    "# Define a custom BERT model with FlashAttention\n",
    "class BertWithFlashAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.flash_attention = FlashSelfAttention(causal=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT encoder outputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Apply FlashAttention on hidden states\n",
    "        attention_out = self.flash_attention(hidden_states)\n",
    "        return attention_out\n",
    "\n",
    "# Define a custom BERT model with FlashAttention-2\n",
    "class BertWithFlashAttention2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.flash_attention2 = FlashAttention2(causal=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT encoder outputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Apply FlashAttention-2 on hidden states\n",
    "        attention_out = self.flash_attention2(hidden_states)\n",
    "        return attention_out\n",
    "\n",
    "# Initialize both models and move them to GPU\n",
    "model_flash = BertWithFlashAttention().to(device)\n",
    "model_flash2 = BertWithFlashAttention2().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TO962OWp2rEl"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_inference(model, inputs, num_runs=100):\n",
    "    # Synchronize GPU to ensure accurate timing\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run multiple inference passes without gradients\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "    # Synchronize again and calculate elapsed time\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvXvQ1Yy2rGr"
   },
   "outputs": [],
   "source": [
    "# Measure inference time for FlashAttention\n",
    "flash_time = run_inference(model_flash, inputs)\n",
    "print(f\"FlashAttention execution time: {flash_time:.4f} seconds\")\n",
    "\n",
    "# Measure inference time for FlashAttention-2\n",
    "flash2_time = run_inference(model_flash2, inputs)\n",
    "print(f\"FlashAttention-2 execution time: {flash2_time:.4f} seconds\")\n",
    "\n",
    "# Verify the output shapes\n",
    "with torch.no_grad():\n",
    "    output_flash = model_flash(inputs['input_ids'], inputs['attention_mask'])\n",
    "    output_flash2 = model_flash2(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "print(\"Output shape with FlashAttention:\", output_flash.shape)\n",
    "print(\"Output shape with FlashAttention-2:\", output_flash2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr-O1Ble3QwP"
   },
   "source": [
    "- The execution times indicate that FlashAttention-2 is generally faster than the original FlashAttention due to its optimized parallelism and better work partitioning.\n",
    "- Both models produce the same output shape, demonstrating that they are functionally equivalent while offering different levels of performance efficiency."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
